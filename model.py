# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B3TbmDWnEIJ8Zry8vgK596cjYHwkXlRH
"""

import pandas as pd
import pickle
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

rf_clf=RandomForestClassifier(n_estimators=100,random_state=42)
#n_estimators = number of trees in the forest.
#Here, it creates 100 decision trees.
#This is a seed for random number generation.
#Every time you run the code with random_state=42, youâ€™ll get the same model results.
knn_clf=KNeighborsClassifier(n_neighbors=5)
#n_neighbors = the number of closest training points to consider when making a prediction.
#Create a KNN classifier that looks at the 5 closest points in the training data to decide the class for a new input.
svm_clf=SVC(probability=True,kernel='rbf',random_state=42)
#By default, an SVM only predicts class labels (like 0 or 1).
#Setting probability=True tells the SVM to also calculate probabilities for each class,

voting_clf=VotingClassifier(estimators=[('rf',rf_clf),('knn',knn_clf),('svm',svm_clf)],voting='soft')

X_train_scaled = pd.read_csv("X_train_scaled.csv")
X_test_scaled = pd.read_csv("X_test_scaled.csv")
y_train = pd.read_csv("y_train.csv").values.ravel()  # flatten to 1D array
y_test = pd.read_csv("y_test.csv").values.ravel()

#Train Random Forest, KNN, and SVM on the same data
#then combine their predictions by averaging probabilities (soft voting) to make the final prediction
voting_clf.fit(X_train_scaled, y_train)

y_pred = voting_clf.predict(X_test_scaled)

accuracy = accuracy_score(y_test, y_pred)
print(f"Test Accuracy: {accuracy:.4f}")
print("Classification Report:")
print(classification_report(y_test,y_pred))

import pickle
with open("voting_classififer.pkl","wb") as f:
  pickle.dump(voting_clf,f)